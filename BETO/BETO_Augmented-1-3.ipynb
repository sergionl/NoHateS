{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTqhrPaSeE-z"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tL1iFjECh3_6",
        "outputId": "3d924c16-5712-4b77-cf92-010dd7c50df7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nlpaug\n",
            "  Downloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (1.23.5)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (2.31.0)\n",
            "Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (4.6.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (3.12.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.11.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2023.3.post1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2023.7.22)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (1.7.1)\n",
            "Installing collected packages: nlpaug\n",
            "Successfully installed nlpaug-1.1.11\n"
          ]
        }
      ],
      "source": [
        "# !pip install transformers -qq\n",
        "# !pip install sentencepiece -qq\n",
        "# !pip install nlpaug"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "La-lPuGjCxbb"
      },
      "source": [
        "## Set Cuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnuQNrQ5Cxi_",
        "outputId": "8f09b8f5-9d6d-4b6d-e78c-fee604ae4bf3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ],
      "source": [
        "import torch\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-pzooQXh5S7"
      },
      "source": [
        "##Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbRc1-_bvdyL"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def to_df(x, y):\n",
        "    d = {\"text\": x, \"label\": y}\n",
        "    return pd.DataFrame(d)\n",
        "\n",
        "def split_3(df, test_size=0.2, valid_size=0.2):\n",
        "    _df = df.copy().sample(frac=1).reset_index()\n",
        "    _df = _df[[\"text\", \"label\"]]\n",
        "\n",
        "    x = df[\"text\"].copy()\n",
        "    y = df[\"label\"].copy()\n",
        "    #split train-test\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size, stratify=y, random_state=SEED)\n",
        "    # split train-valid\n",
        "    x, y = x_train, y_train\n",
        "    x_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size=valid_size, stratify=y, random_state=SEED)\n",
        "    return to_df(x_train, y_train), to_df(x_valid, y_valid), to_df(x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wiuxFidnMx58"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from itertools import chain\n",
        "import nlpaug.augmenter.char as nac\n",
        "import nlpaug.augmenter.word as naw\n",
        "from nlpaug.util import Action\n",
        "\n",
        "\n",
        "alpha_common_error = 0.10\n",
        "alpha_common_error_char = 0.05\n",
        "aug1_OCR = nac.OcrAug(aug_word_p=alpha_common_error)\n",
        "aug2_Rins = nac.RandomCharAug(action=\"insert\", aug_word_p=alpha_common_error, aug_char_min=1, aug_char_max=1, aug_char_p=alpha_common_error_char)\n",
        "aug3_Rsub = nac.RandomCharAug(action=\"substitute\", aug_word_p=alpha_common_error, aug_char_min=1, aug_char_max=1, aug_char_p=alpha_common_error_char)\n",
        "aug4_Rswa = nac.RandomCharAug(action=\"swap\", aug_word_p=alpha_common_error,aug_char_min=1, aug_char_max=1, aug_char_p=alpha_common_error_char) #\n",
        "aug5_Rdel = nac.RandomCharAug(action=\"delete\", aug_word_p=alpha_common_error, aug_char_min=1, aug_char_max=1, aug_char_p=alpha_common_error_char)\n",
        "aug6_Kb = nac.KeyboardAug(aug_word_p=alpha_common_error)\n",
        "aug7_Split = naw.SplitAug(aug_p=alpha_common_error)\n",
        "\n",
        "\n",
        "def text2augment(text, m):\n",
        "    output = [text, ]\n",
        "\n",
        "    temp = random.sample(range(0, 7), m - 1)\n",
        "\n",
        "    if 0 in temp:\n",
        "        output.append( *aug1_OCR.augment(text))\n",
        "    if 1 in temp:\n",
        "        output.append( *aug2_Rins.augment(text))\n",
        "    if 2 in temp:\n",
        "        output.append( *aug3_Rsub.augment(text))\n",
        "    if 3 in temp:\n",
        "        output.append( *aug4_Rswa.augment(text))\n",
        "    if 4 in temp:\n",
        "        output.append( *aug5_Rdel.augment(text))\n",
        "    if 5 in temp:\n",
        "        output.append( *aug6_Kb.augment(text))\n",
        "    if 6 in temp:\n",
        "        output.append( *aug7_Split.augment(text))\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "def aug_replicate(y_labels):\n",
        "    return list(chain(* [[y]*(1 if y == 0 else 3) for y in y_labels] ))\n",
        "\n",
        "def aug_text(x_text, y_labels):\n",
        "    x_text = [ text2augment(x, 1 if y == 0 else 3) for x, y in zip(x_text, y_labels)]\n",
        "    return pd.Series(list(chain(*x_text)), index=None)\n",
        "\n",
        "def split_3_aug(df, test_size=0.2, valid_size=0.2):\n",
        "    _df = df.copy().sample(frac=1).reset_index()\n",
        "    _df = _df[[\"text\", \"label\"]]\n",
        "\n",
        "    x = _df[\"text\"].copy()\n",
        "    y = _df[\"label\"].copy()\n",
        "    #split train-test\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size, stratify=y, random_state=SEED)\n",
        "    # augment\n",
        "    # x_test = aug_text(x_test, y_test)\n",
        "    # y_test = aug_replicate(y_test)\n",
        "    # split train-valid\n",
        "    x, y = x_train, y_train\n",
        "    x_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size=valid_size, stratify=y, random_state=SEED)\n",
        "    # augment\n",
        "    x_train = aug_text(x_train, y_train)\n",
        "    y_train = aug_replicate(y_train)\n",
        "    x_valid = aug_text(x_valid, y_valid)\n",
        "    y_valid = aug_replicate(y_valid)\n",
        "\n",
        "    print(x_valid.shape)\n",
        "    print(\"DONE\")\n",
        "    print(len(y_valid))\n",
        "\n",
        "    print(x_train.shape)\n",
        "    print(\"DONE\")\n",
        "    print(len(y_train))\n",
        "\n",
        "    return to_df(x_train, y_train), to_df(x_valid, y_valid), to_df(x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTMXPNE2W6NJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "tname_data = \"./hsd_merge_cleaned_lowered\"\n",
        "data = pd.read_csv(f\"{tname_data}.csv\")\n",
        "\n",
        "train, valid, test = split_3_aug(data)\n",
        "\n",
        "X_train = train['text']\n",
        "y_train = train['label']\n",
        "\n",
        "X_valid = valid['text']\n",
        "y_valid = valid['label']\n",
        "\n",
        "X_test = test['text']\n",
        "y_test = test['label']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHV2PrVfiBLg"
      },
      "source": [
        "# Extract feature by using BETO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MreD1ev6AgXM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from glob import glob\n",
        "\n",
        "train_sentences = list(train['text'].values)\n",
        "train_labels = list(train['label'].values)\n",
        "\n",
        "valid_sentences = list(valid['text'].values)\n",
        "valid_labels = list(valid['label'].values)\n",
        "\n",
        "test_sentences = list(test['text'].values)\n",
        "test_labels = list(test['label'].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNVMN4YPiNjp"
      },
      "source": [
        "Load tokenizer of BETO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gI8SPqPcA8BS"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('dccuchile/bert-base-spanish-wwm-cased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PNp5aRVA-Hb",
        "outputId": "dbb89ca9-04db-4a40-c425-e036d64eb3fb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(17, 404),\n",
              " (15, 380),\n",
              " (23, 354),\n",
              " (18, 344),\n",
              " (24, 343),\n",
              " (19, 340),\n",
              " (22, 337),\n",
              " (21, 336),\n",
              " (20, 335),\n",
              " (14, 328),\n",
              " (12, 325),\n",
              " (13, 325),\n",
              " (16, 325),\n",
              " (26, 310),\n",
              " (9, 302),\n",
              " (11, 298),\n",
              " (10, 293),\n",
              " (25, 277),\n",
              " (29, 271),\n",
              " (27, 270),\n",
              " (28, 262),\n",
              " (31, 210),\n",
              " (7, 210),\n",
              " (32, 201),\n",
              " (30, 200),\n",
              " (8, 192),\n",
              " (33, 189),\n",
              " (34, 148),\n",
              " (6, 144),\n",
              " (36, 137),\n",
              " (38, 121),\n",
              " (53, 117),\n",
              " (39, 117),\n",
              " (35, 117),\n",
              " (37, 117),\n",
              " (40, 116),\n",
              " (5, 115),\n",
              " (42, 112),\n",
              " (50, 111),\n",
              " (43, 109),\n",
              " (54, 109),\n",
              " (41, 106),\n",
              " (44, 106),\n",
              " (49, 105),\n",
              " (45, 102),\n",
              " (48, 101),\n",
              " (52, 100),\n",
              " (56, 98),\n",
              " (51, 98),\n",
              " (46, 97),\n",
              " (47, 92),\n",
              " (55, 85),\n",
              " (58, 85),\n",
              " (57, 84),\n",
              " (4, 67),\n",
              " (59, 61),\n",
              " (60, 58),\n",
              " (61, 55),\n",
              " (64, 38),\n",
              " (62, 36),\n",
              " (65, 33),\n",
              " (3, 31),\n",
              " (66, 28),\n",
              " (63, 26),\n",
              " (67, 17),\n",
              " (69, 13),\n",
              " (68, 12),\n",
              " (2, 7),\n",
              " (72, 6),\n",
              " (70, 6),\n",
              " (1, 4),\n",
              " (71, 2),\n",
              " (73, 1),\n",
              " (74, 1),\n",
              " (81, 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ],
      "source": [
        "#choose max_length for phobert model based on the input length\n",
        "\n",
        "max_length = 0\n",
        "list_len=[]\n",
        "for sentence in train_sentences:\n",
        "    length = len(tokenizer.tokenize(sentence))\n",
        "    list_len.append(length)\n",
        "\n",
        "from collections import Counter\n",
        "Counter(list_len).most_common(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dX5lBYCTD9bU"
      },
      "outputs": [],
      "source": [
        "# Encode train label\n",
        "\n",
        "from sklearn import preprocessing\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(train_labels)\n",
        "encoded_train_labels = le.transform(train_labels)\n",
        "encoded_valid_labels = le.transform(valid_labels)\n",
        "encoded_test_labels = le.transform(test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9XdWNkpFBgH",
        "outputId": "9b7d2a08-7271-42c6-9741-f5e557cbc58f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  maripan ctm siempre mandandose cagas\n",
            "Token IDs: tensor([    4,  4352,  5249,  1016, 30940, 30943,  2032,  4521, 30935,  2610,\n",
            "         1285,  1723,     5,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
            "       device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "# Tokens IDs tensor\n",
        "\n",
        "def encoder_generator(sentences,labels):\n",
        "\n",
        "    sent_index = []\n",
        "    input_ids = []\n",
        "    attention_masks =[]\n",
        "\n",
        "    for index,sent in enumerate(sentences):\n",
        "\n",
        "        sent_index.append(index)\n",
        "\n",
        "        encoded_dict = tokenizer.encode_plus(sent,\n",
        "                                             add_special_tokens=True,\n",
        "                                             max_length=50,\n",
        "                                             padding='max_length',\n",
        "                                             truncation = True,\n",
        "                                             return_attention_mask=True,\n",
        "                                             return_tensors='pt')\n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "    input_ids = torch.cat(input_ids,dim=0).cuda()\n",
        "    attention_masks = torch.cat(attention_masks,dim=0).cuda()\n",
        "    labels = torch.tensor(labels).cuda()\n",
        "    sent_index = torch.tensor(sent_index).cuda()\n",
        "\n",
        "    return sent_index,input_ids,attention_masks,labels\n",
        "\n",
        "train_sent_index,train_input_ids,train_attention_masks,train_encoded_label_tensors = encoder_generator(train_sentences,encoded_train_labels)\n",
        "valid_sent_index,valid_input_ids,valid_attention_masks,valid_encoded_label_tensors = encoder_generator(valid_sentences,encoded_valid_labels)\n",
        "print('Original: ', train_sentences[0])\n",
        "print('Token IDs:', train_input_ids[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BYbqZ7NMMWw",
        "outputId": "8fb94d7c-54ef-4a43-f677-6908d129c9ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train data samples is 11413\n",
            "valid data samples is 2854\n"
          ]
        }
      ],
      "source": [
        "# Connvert train, dev input by using TensorDataset\n",
        "\n",
        "from torch.utils.data import TensorDataset,random_split\n",
        "\n",
        "train_dataset = TensorDataset(train_input_ids,train_attention_masks,train_encoded_label_tensors)\n",
        "valid_dataset = TensorDataset(valid_input_ids,valid_attention_masks,valid_encoded_label_tensors)\n",
        "\n",
        "print('train data samples is {}'.format(len(train_dataset)))\n",
        "print(\"valid data samples is {}\".format(len(valid_dataset)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Em9Q1v0BMZZX"
      },
      "outputs": [],
      "source": [
        "# Set cuda by using device\n",
        "\n",
        "from torch.utils.data import DataLoader,RandomSampler,SequentialSampler\n",
        "\n",
        "bs=128\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_data_loader = DataLoader(train_dataset,\n",
        "                              sampler=RandomSampler(train_dataset),\n",
        "                              batch_size=bs)\n",
        "valid_data_loader = DataLoader(valid_dataset,\n",
        "                              sampler=RandomSampler(valid_dataset),\n",
        "                              batch_size=bs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEtD7kn4jbTl"
      },
      "source": [
        "Load model BETO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTo_bpEeMqgr"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "\n",
        "EMBEDDING_DIM = 768\n",
        "N_FILTERS = 32\n",
        "FILTER_SIZES = [1,2,3,5]\n",
        "OUTPUT_DIM = len(le.classes_)\n",
        "DROPOUT = 0.1\n",
        "PAD_IDX = tokenizer.pad_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00f8RDsuGXi0"
      },
      "outputs": [],
      "source": [
        "gg = {0: 0, 1: 1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQiHRn2zMdIJ",
        "outputId": "af570bbb-6866-4217-ee38-a00f9a77aa7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModel\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoModel\n",
        "\n",
        "# beto = AutoModel.from_pretrained('dccuchile/bert-base-spanish-wwm-cased')\n",
        "beto = AutoModelForSequenceClassification.from_pretrained(\n",
        "    'dccuchile/bert-base-spanish-wwm-cased', num_labels=OUTPUT_DIM, id2label=gg, label2id=gg\n",
        ")\n",
        "beto = beto.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZIPUdZfHUYb",
        "outputId": "47aa7bc5-2ed3-4e63-8880-e2866b64a6d6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(31002, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ],
      "source": [
        "beto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITmi7WZDMsl6"
      },
      "outputs": [],
      "source": [
        "# Optimizer and criterion\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "model_parameters = list(beto.parameters())\n",
        "\n",
        "optimizer = optim.Adam(model_parameters,lr=2e-5,eps=1e-8)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "criterion = criterion.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyp8HhAxMvQH"
      },
      "outputs": [],
      "source": [
        "# Calculate accuracy per batch during train\n",
        "\n",
        "def categorical_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
        "    correct = max_preds.squeeze(1).eq(y)\n",
        "    return correct.sum() / torch.FloatTensor([y.shape[0]]).cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrVcBEVoMxov"
      },
      "outputs": [],
      "source": [
        "# Def for training\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train():\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    beto.train()\n",
        "\n",
        "    for batch in tqdm(train_data_loader):\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        predictions = beto(b_input_ids,b_input_mask)[\"logits\"]\n",
        "\n",
        "        loss = criterion(predictions, b_labels)\n",
        "\n",
        "        acc = categorical_accuracy(predictions, b_labels)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(train_data_loader), epoch_acc / len(train_data_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTQGFfnFMz-d"
      },
      "outputs": [],
      "source": [
        "# Class for predict label\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def predictions_labels(preds,labels):\n",
        "    pred = np.argmax(preds,axis=1).flatten()\n",
        "    label = labels.flatten()\n",
        "    return pred,label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_4Y1RhlM17e"
      },
      "outputs": [],
      "source": [
        "# Evaluate loss, acc  and f1-macro\n",
        "\n",
        "from sklearn.metrics import classification_report,accuracy_score,f1_score\n",
        "def eval():\n",
        "    epoch_loss = 0\n",
        "\n",
        "    total_predictions = []\n",
        "    total_true = []\n",
        "\n",
        "    all_true_labels = []\n",
        "    all_pred_labels = []\n",
        "\n",
        "    beto.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for batch in tqdm(valid_data_loader):\n",
        "            b_input_ids = batch[0].to(device)\n",
        "            b_input_mask = batch[1].to(device)\n",
        "            b_labels = batch[2].to(device)\n",
        "\n",
        "            predictions = beto(b_input_ids,b_input_mask)[\"logits\"]\n",
        "\n",
        "            loss = criterion(predictions, b_labels)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            predictions = predictions.detach().cpu().numpy()\n",
        "\n",
        "            label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "            pred,true = predictions_labels(predictions,label_ids)\n",
        "\n",
        "            all_pred_labels.extend(pred)\n",
        "            all_true_labels.extend(true)\n",
        "\n",
        "    print(classification_report(all_pred_labels,all_true_labels))\n",
        "    avg_val_accuracy = accuracy_score(all_pred_labels,all_true_labels)\n",
        "    macro_f1_score = f1_score(all_pred_labels,all_true_labels,average='macro')\n",
        "\n",
        "    avg_val_loss = epoch_loss/len(valid_data_loader)\n",
        "\n",
        "    print(\"accuracy = {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    return avg_val_loss,avg_val_accuracy,macro_f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICyenO-tM35S"
      },
      "outputs": [],
      "source": [
        "# Time for training\n",
        "\n",
        "import time\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OrAOzamNC0e"
      },
      "outputs": [],
      "source": [
        "# # Set device and gpu\n",
        "\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# n_gpu = torch.cuda.device_count()\n",
        "# torch.cuda.get_device_name(0)\n",
        "\n",
        "# beto.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbJBkClEkam2"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZuaZEzbPMx5-"
      },
      "outputs": [],
      "source": [
        "tempname = \"./beto_aug_model_1-1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfHllZjgM5uw",
        "outputId": "d0c5e0c4-bb4a-48ae-9eb0-794ba42bb63a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 90/90 [01:26<00:00,  1.03it/s]\n",
            "100%|██████████| 23/23 [00:07<00:00,  3.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.88      0.92      2596\n",
            "           1       0.38      0.75      0.50       258\n",
            "\n",
            "    accuracy                           0.87      2854\n",
            "   macro avg       0.68      0.81      0.71      2854\n",
            "weighted avg       0.92      0.87      0.89      2854\n",
            "\n",
            "accuracy = 0.87\n",
            "model saved\n",
            "Epoch: 01 | Epoch Time: 1m 34s\n",
            "\tTrain Loss: 0.392 | Train acc: 83.48%\n",
            "\t Val. Loss: 0.323 |  Val. acc: 86.69%\n",
            "\t Val. Loss: 0.323 |  Val. F1: 71.35%\n",
            "=============Epoch Ended==============\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 90/90 [01:26<00:00,  1.04it/s]\n",
            "100%|██████████| 23/23 [00:07<00:00,  3.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.89      0.93      2574\n",
            "           1       0.43      0.78      0.55       280\n",
            "\n",
            "    accuracy                           0.88      2854\n",
            "   macro avg       0.70      0.83      0.74      2854\n",
            "weighted avg       0.92      0.88      0.89      2854\n",
            "\n",
            "accuracy = 0.88\n",
            "model saved\n",
            "Epoch: 02 | Epoch Time: 1m 33s\n",
            "\tTrain Loss: 0.279 | Train acc: 88.60%\n",
            "\t Val. Loss: 0.314 |  Val. acc: 87.67%\n",
            "\t Val. Loss: 0.314 |  Val. F1: 74.09%\n",
            "=============Epoch Ended==============\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 90/90 [01:26<00:00,  1.04it/s]\n",
            "100%|██████████| 23/23 [00:07<00:00,  3.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.90      0.92      2457\n",
            "           1       0.54      0.69      0.60       397\n",
            "\n",
            "    accuracy                           0.87      2854\n",
            "   macro avg       0.74      0.79      0.76      2854\n",
            "weighted avg       0.89      0.87      0.88      2854\n",
            "\n",
            "accuracy = 0.87\n",
            "model saved\n",
            "Epoch: 03 | Epoch Time: 1m 34s\n",
            "\tTrain Loss: 0.210 | Train acc: 91.68%\n",
            "\t Val. Loss: 0.345 |  Val. acc: 87.35%\n",
            "\t Val. Loss: 0.345 |  Val. F1: 76.30%\n",
            "=============Epoch Ended==============\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 90/90 [01:26<00:00,  1.04it/s]\n",
            "100%|██████████| 23/23 [00:07<00:00,  3.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.93      0.91      2228\n",
            "           1       0.69      0.56      0.62       626\n",
            "\n",
            "    accuracy                           0.85      2854\n",
            "   macro avg       0.79      0.75      0.76      2854\n",
            "weighted avg       0.84      0.85      0.84      2854\n",
            "\n",
            "accuracy = 0.85\n",
            "Epoch: 04 | Epoch Time: 1m 33s\n",
            "\tTrain Loss: 0.129 | Train acc: 95.40%\n",
            "\t Val. Loss: 0.388 |  Val. acc: 84.86%\n",
            "\t Val. Loss: 0.388 |  Val. F1: 76.23%\n",
            "=============Epoch Ended==============\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 90/90 [01:25<00:00,  1.05it/s]\n",
            "100%|██████████| 23/23 [00:07<00:00,  3.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.90      0.92      2436\n",
            "           1       0.53      0.65      0.58       418\n",
            "\n",
            "    accuracy                           0.86      2854\n",
            "   macro avg       0.73      0.77      0.75      2854\n",
            "weighted avg       0.88      0.86      0.87      2854\n",
            "\n",
            "accuracy = 0.86\n",
            "Epoch: 05 | Epoch Time: 1m 33s\n",
            "\tTrain Loss: 0.071 | Train acc: 97.65%\n",
            "\t Val. Loss: 0.522 |  Val. acc: 86.48%\n",
            "\t Val. Loss: 0.522 |  Val. F1: 75.12%\n",
            "=============Epoch Ended==============\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 90/90 [01:25<00:00,  1.05it/s]\n",
            "100%|██████████| 23/23 [00:07<00:00,  3.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.92      0.92      2328\n",
            "           1       0.64      0.62      0.63       526\n",
            "\n",
            "    accuracy                           0.87      2854\n",
            "   macro avg       0.78      0.77      0.77      2854\n",
            "weighted avg       0.86      0.87      0.86      2854\n",
            "\n",
            "accuracy = 0.87\n",
            "model saved\n",
            "Epoch: 06 | Epoch Time: 1m 33s\n",
            "\tTrain Loss: 0.046 | Train acc: 98.41%\n",
            "\t Val. Loss: 0.506 |  Val. acc: 86.55%\n",
            "\t Val. Loss: 0.506 |  Val. F1: 77.32%\n",
            "=============Epoch Ended==============\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 90/90 [01:26<00:00,  1.04it/s]\n",
            "100%|██████████| 23/23 [00:07<00:00,  3.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.89      0.93      2538\n",
            "           1       0.46      0.74      0.57       316\n",
            "\n",
            "    accuracy                           0.88      2854\n",
            "   macro avg       0.71      0.82      0.75      2854\n",
            "weighted avg       0.91      0.88      0.89      2854\n",
            "\n",
            "accuracy = 0.88\n",
            "Epoch: 07 | Epoch Time: 1m 33s\n",
            "\tTrain Loss: 0.035 | Train acc: 98.71%\n",
            "\t Val. Loss: 0.605 |  Val. acc: 87.60%\n",
            "\t Val. Loss: 0.605 |  Val. F1: 74.90%\n",
            "=============Epoch Ended==============\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 90/90 [01:25<00:00,  1.05it/s]\n",
            "100%|██████████| 23/23 [00:07<00:00,  3.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.92      0.90      2280\n",
            "           1       0.62      0.55      0.59       574\n",
            "\n",
            "    accuracy                           0.84      2854\n",
            "   macro avg       0.76      0.73      0.74      2854\n",
            "weighted avg       0.84      0.84      0.84      2854\n",
            "\n",
            "accuracy = 0.84\n",
            "Epoch: 08 | Epoch Time: 1m 33s\n",
            "\tTrain Loss: 0.026 | Train acc: 99.10%\n",
            "\t Val. Loss: 0.660 |  Val. acc: 84.30%\n",
            "\t Val. Loss: 0.660 |  Val. F1: 74.46%\n",
            "=============Epoch Ended==============\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 90/90 [01:25<00:00,  1.05it/s]\n",
            "100%|██████████| 23/23 [00:07<00:00,  3.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.92      0.91      2322\n",
            "           1       0.63      0.60      0.62       532\n",
            "\n",
            "    accuracy                           0.86      2854\n",
            "   macro avg       0.77      0.76      0.77      2854\n",
            "weighted avg       0.86      0.86      0.86      2854\n",
            "\n",
            "accuracy = 0.86\n",
            "Epoch: 09 | Epoch Time: 1m 33s\n",
            "\tTrain Loss: 0.022 | Train acc: 99.30%\n",
            "\t Val. Loss: 0.622 |  Val. acc: 86.05%\n",
            "\t Val. Loss: 0.622 |  Val. F1: 76.60%\n",
            "=============Epoch Ended==============\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 90/90 [01:25<00:00,  1.05it/s]\n",
            "100%|██████████| 23/23 [00:07<00:00,  3.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.89      0.93      2528\n",
            "           1       0.46      0.72      0.57       326\n",
            "\n",
            "    accuracy                           0.87      2854\n",
            "   macro avg       0.71      0.81      0.75      2854\n",
            "weighted avg       0.90      0.87      0.88      2854\n",
            "\n",
            "accuracy = 0.87\n",
            "Epoch: 10 | Epoch Time: 1m 33s\n",
            "\tTrain Loss: 0.016 | Train acc: 99.51%\n",
            "\t Val. Loss: 0.718 |  Val. acc: 87.32%\n",
            "\t Val. Loss: 0.718 |  Val. F1: 74.58%\n",
            "=============Epoch Ended==============\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 90/90 [01:25<00:00,  1.05it/s]\n",
            "100%|██████████| 23/23 [00:07<00:00,  3.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.93      0.91      2228\n",
            "           1       0.69      0.56      0.62       626\n",
            "\n",
            "    accuracy                           0.85      2854\n",
            "   macro avg       0.79      0.75      0.76      2854\n",
            "weighted avg       0.84      0.85      0.84      2854\n",
            "\n",
            "accuracy = 0.85\n",
            "Epoch: 11 | Epoch Time: 1m 33s\n",
            "\tTrain Loss: 0.014 | Train acc: 99.52%\n",
            "\t Val. Loss: 0.759 |  Val. acc: 84.86%\n",
            "\t Val. Loss: 0.759 |  Val. F1: 76.23%\n",
            "=============Epoch Ended==============\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 90/90 [01:26<00:00,  1.05it/s]\n",
            "100%|██████████| 23/23 [00:07<00:00,  3.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.93      0.92      2297\n",
            "           1       0.67      0.61      0.63       557\n",
            "\n",
            "    accuracy                           0.86      2854\n",
            "   macro avg       0.79      0.77      0.78      2854\n",
            "weighted avg       0.86      0.86      0.86      2854\n",
            "\n",
            "accuracy = 0.86\n",
            "model saved\n",
            "Epoch: 12 | Epoch Time: 1m 33s\n",
            "\tTrain Loss: 0.014 | Train acc: 99.60%\n",
            "\t Val. Loss: 0.770 |  Val. acc: 86.37%\n",
            "\t Val. Loss: 0.770 |  Val. F1: 77.55%\n",
            "=============Epoch Ended==============\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 90/90 [01:26<00:00,  1.05it/s]\n",
            "100%|██████████| 23/23 [00:07<00:00,  3.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.92      0.92      2347\n",
            "           1       0.63      0.63      0.63       507\n",
            "\n",
            "    accuracy                           0.87      2854\n",
            "   macro avg       0.77      0.77      0.77      2854\n",
            "weighted avg       0.87      0.87      0.87      2854\n",
            "\n",
            "accuracy = 0.87\n",
            "Epoch: 13 | Epoch Time: 1m 33s\n",
            "\tTrain Loss: 0.014 | Train acc: 99.49%\n",
            "\t Val. Loss: 0.689 |  Val. acc: 86.79%\n",
            "\t Val. Loss: 0.689 |  Val. F1: 77.41%\n",
            "=============Epoch Ended==============\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 90/90 [01:25<00:00,  1.05it/s]\n",
            "100%|██████████| 23/23 [00:07<00:00,  3.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.92      0.91      2276\n",
            "           1       0.66      0.58      0.62       578\n",
            "\n",
            "    accuracy                           0.86      2854\n",
            "   macro avg       0.78      0.75      0.77      2854\n",
            "weighted avg       0.85      0.86      0.85      2854\n",
            "\n",
            "accuracy = 0.86\n",
            "Epoch: 14 | Epoch Time: 1m 33s\n",
            "\tTrain Loss: 0.015 | Train acc: 99.48%\n",
            "\t Val. Loss: 0.706 |  Val. acc: 85.56%\n",
            "\t Val. Loss: 0.706 |  Val. F1: 76.57%\n",
            "=============Epoch Ended==============\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 90/90 [01:25<00:00,  1.05it/s]\n",
            "100%|██████████| 23/23 [00:07<00:00,  3.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.91      0.92      2393\n",
            "           1       0.59      0.66      0.62       461\n",
            "\n",
            "    accuracy                           0.87      2854\n",
            "   macro avg       0.76      0.78      0.77      2854\n",
            "weighted avg       0.88      0.87      0.87      2854\n",
            "\n",
            "accuracy = 0.87\n",
            "Epoch: 15 | Epoch Time: 1m 33s\n",
            "\tTrain Loss: 0.014 | Train acc: 99.41%\n",
            "\t Val. Loss: 0.781 |  Val. acc: 87.21%\n",
            "\t Val. Loss: 0.781 |  Val. F1: 77.32%\n",
            "=============Epoch Ended==============\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 90/90 [01:25<00:00,  1.05it/s]\n",
            "100%|██████████| 23/23 [00:07<00:00,  3.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.91      0.92      2396\n",
            "           1       0.58      0.64      0.61       458\n",
            "\n",
            "    accuracy                           0.87      2854\n",
            "   macro avg       0.76      0.78      0.77      2854\n",
            "weighted avg       0.87      0.87      0.87      2854\n",
            "\n",
            "accuracy = 0.87\n",
            "Epoch: 16 | Epoch Time: 1m 33s\n",
            "\tTrain Loss: 0.009 | Train acc: 99.71%\n",
            "\t Val. Loss: 0.729 |  Val. acc: 86.83%\n",
            "\t Val. Loss: 0.729 |  Val. F1: 76.57%\n",
            "=============Epoch Ended==============\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 90/90 [01:25<00:00,  1.05it/s]\n",
            "100%|██████████| 23/23 [00:07<00:00,  3.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.91      0.92      2360\n",
            "           1       0.60      0.62      0.61       494\n",
            "\n",
            "    accuracy                           0.86      2854\n",
            "   macro avg       0.76      0.77      0.76      2854\n",
            "weighted avg       0.86      0.86      0.86      2854\n",
            "\n",
            "accuracy = 0.86\n",
            "Epoch: 17 | Epoch Time: 1m 33s\n",
            "\tTrain Loss: 0.012 | Train acc: 99.57%\n",
            "\t Val. Loss: 0.692 |  Val. acc: 86.33%\n",
            "\t Val. Loss: 0.692 |  Val. F1: 76.40%\n",
            "=============Epoch Ended==============\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 90/90 [01:25<00:00,  1.05it/s]\n",
            "100%|██████████| 23/23 [00:07<00:00,  3.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.91      0.92      2427\n",
            "           1       0.56      0.66      0.61       427\n",
            "\n",
            "    accuracy                           0.87      2854\n",
            "   macro avg       0.75      0.79      0.76      2854\n",
            "weighted avg       0.88      0.87      0.88      2854\n",
            "\n",
            "accuracy = 0.87\n",
            "Epoch: 18 | Epoch Time: 1m 33s\n",
            "\tTrain Loss: 0.013 | Train acc: 99.48%\n",
            "\t Val. Loss: 0.756 |  Val. acc: 87.07%\n",
            "\t Val. Loss: 0.756 |  Val. F1: 76.40%\n",
            "=============Epoch Ended==============\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 90/90 [01:25<00:00,  1.05it/s]\n",
            "100%|██████████| 23/23 [00:07<00:00,  3.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.90      0.92      2452\n",
            "           1       0.53      0.67      0.59       402\n",
            "\n",
            "    accuracy                           0.87      2854\n",
            "   macro avg       0.74      0.79      0.76      2854\n",
            "weighted avg       0.89      0.87      0.88      2854\n",
            "\n",
            "accuracy = 0.87\n",
            "Epoch: 19 | Epoch Time: 1m 33s\n",
            "\tTrain Loss: 0.010 | Train acc: 99.63%\n",
            "\t Val. Loss: 0.747 |  Val. acc: 86.97%\n",
            "\t Val. Loss: 0.747 |  Val. F1: 75.68%\n",
            "=============Epoch Ended==============\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 90/90 [01:26<00:00,  1.05it/s]\n",
            "100%|██████████| 23/23 [00:07<00:00,  3.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.91      0.92      2406\n",
            "           1       0.57      0.64      0.60       448\n",
            "\n",
            "    accuracy                           0.87      2854\n",
            "   macro avg       0.75      0.78      0.76      2854\n",
            "weighted avg       0.87      0.87      0.87      2854\n",
            "\n",
            "accuracy = 0.87\n",
            "Epoch: 20 | Epoch Time: 1m 33s\n",
            "\tTrain Loss: 0.005 | Train acc: 99.80%\n",
            "\t Val. Loss: 0.827 |  Val. acc: 86.69%\n",
            "\t Val. Loss: 0.827 |  Val. F1: 76.13%\n",
            "=============Epoch Ended==============\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "epochs = 20\n",
        "\n",
        "best_macro_f1 = float('0')\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    start_time = time.time()\n",
        "    train_loss,train_acc = train()\n",
        "    valid_loss,valid_acc,macro_f1 = eval()\n",
        "    end_time = time.time()\n",
        "\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    if macro_f1 > best_macro_f1:\n",
        "        best_macro_f1 = macro_f1\n",
        "        torch.save(beto,tempname + '_task2a_2.pt')\n",
        "        print(\"model saved\")\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. acc: {valid_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. F1: {macro_f1*100:.2f}%')\n",
        "    print('=============Epoch Ended==============')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KyQlcVfnCWHw"
      },
      "outputs": [],
      "source": [
        "# Save BETO and CNN\n",
        "\n",
        "# torch.save(beto,tempname + 'module2_part1.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCzo5AKq8lka"
      },
      "source": [
        "## EVALUATING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftons15Hw8lB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b4e3bcb-3105-48df-8ac7-e60ec7645edf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(31002, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ],
      "source": [
        "# Load phobert and cnn\n",
        "\n",
        "import torch\n",
        "beto = torch.load(tempname + '_task2a_2.pt')\n",
        "beto.eval()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6sFPdMFlPRp"
      },
      "source": [
        "Predict label from true label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9Co-AvE_Ehq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0540befe-5977-4fb5-9c8a-ed109a0df8a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28/28 [00:09<00:00,  2.94it/s]\n"
          ]
        }
      ],
      "source": [
        "test_sent_index, test_input_ids, test_attention_masks, test_encoded_label_tensors = encoder_generator(test_sentences,encoded_test_labels)\n",
        "test_dataset = TensorDataset(test_input_ids,test_attention_masks,test_encoded_label_tensors)\n",
        "\n",
        "test_data_loader = DataLoader(test_dataset,\n",
        "                              sampler=RandomSampler(test_dataset),\n",
        "                              batch_size=bs)\n",
        "\n",
        "all_pred_labels = []\n",
        "all_true_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "  for batch in tqdm(test_data_loader):\n",
        "    b_input_ids = batch[0].to(device)\n",
        "    b_input_mask = batch[1].to(device)\n",
        "    b_labels = batch[2].to(device)\n",
        "\n",
        "    predictions = beto(b_input_ids,b_input_mask)[\"logits\"]\n",
        "\n",
        "\n",
        "    predictions = predictions.detach().cpu().numpy()\n",
        "\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    pred, true = predictions_labels(predictions, label_ids)\n",
        "\n",
        "    all_pred_labels.extend(pred)\n",
        "    all_true_labels.extend(true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESaLntjdAfYF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "187e1e66-1b5e-4b23-ad4b-d08034364bd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9125    0.9038    0.9082      2932\n",
            "           1     0.5747    0.6000    0.5871       635\n",
            "\n",
            "    accuracy                         0.8497      3567\n",
            "   macro avg     0.7436    0.7519    0.7476      3567\n",
            "weighted avg     0.8524    0.8497    0.8510      3567\n",
            "\n"
          ]
        }
      ],Z
      "source": [
        "# The final score in the test set (classification report)\n",
        "\n",
        "# print(classification_report(all_pred_labels,all_true_labels, digits = 4))\n",
        "print(classification_report(all_true_labels,all_pred_labels, digits = 4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zqXgAK1zy6Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ac8f5b5-4c6a-4205-afe6-dc42ea71cae3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2635,  297],\n",
              "       [ 211,  424]])"
            ]
          },
          "metadata": {},
          "execution_count": 147
        }
      ],
      "source": [
        "# Confusion matrix in thetest set\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(all_true_labels, all_pred_labels)\n",
        "cm"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}